For the week of 9/30
Worked with Michael on adding the dots grid and lines that do not connect to the edges of the original image.
Learned the basics for Matlab, such as formating for loops and if statements.

For the week of 10/7
Worked with Michael on the psuedo code for A* while learning how A* functions
Learned more about Matlab and its ways to use and manipulate Java functionality
Discussed with Michael some further steps as after this point we will be working on separate parts of the project, 
	such as plans for how I will connect elevators and stairs to nodes and how will movement through those work
	as well as what ways can we get A* to work (using a .jar file or running A* through Java instead).

For the week of 10/14
Researched ways to find elevators and stairs through use of image recogntion using Matlab, found two hopeful classes: normxcorr2 and 
	vision.TemplateMatcher
After extensive testing with the classes, such as using alternative images and rotating and fliping images, I determined that the best ways that 
	those classes could be used was either having a image (the elevator or stairs) that is directly from the bigger image (blueprints) or use 
	the location of where the image would be that we want to compare with. While it is possible that I have been misunderstanding how to use
	these classes, especially with my inexperience with Matlab, I have concluded that these paths are dead ends, especially since each elevator 
	and stairs for each floor of the blueprints are slightly different size, orentaiton, or some stairs have extra steps, meaning that its 
	impossible to get the perfect copy of what is in the actual image, which is what these classes are asking for, and be able to use one image as
	a base to search through the rest of the images.
Next week I'll be discussing with the others some alternative methods to getting the elevators and stairs to connect. I believe that the most simplest
	method we could do is having the user label areas as elevators and use their location on the image to connect between their location on different
	floors.

For the week of 10/21
Decided as a group to use the Cascade Object Detector, but first I had to train it.
It took me some time to learn how the training works, with using the Training Image Labler app and getting the function to work,
	but most of my time was tweeking it and adding more images to compare to.
As you can see in Jonathan-1-Week4 image in the screenshot folder, the first few tries with the trainer were close, but flawed, but in Jonathan-2-Week4
	both bounding boxes are in the correct places, but not aligned. Same goes for all in the Glennan folder, but they can at times have more than 
	one bounding box in it at once. Either way, this method to find the elevators looks hopeful.
The bounding boxes will be fixed by adding more images to the training that I will have done by this weekend, same goes for the stairs which I have worked
	on but are not in any presentable shape, since running it has nothing close to correct showing up, probably due to the fact that I need more 
	images involving every rotation and variation that the stairs could be, which are alot more than any variation that the elevators could be. 
	Getting to this point took me some time, but now that I understand how this method works, I can manipulate it better.
Uploaded are the xml file of ElevatorDetector which is the trained file that is used in the m file. In the m file you can change the image file to see how  
	the different images are effected by the Cascade Object Detector.

For the week of 10/28
Even with the break, work was slow this week, as I got sick over the fall break
I am as close as I seem I can be with the elevator detector, will tweek it a little bit maybe as only one image I noticed is not detecting an elevator.
	We decided that even if we could not get the whole elevator in the bounding box, it would be fine as if we removed it, we could add a node there
	that would not be blocked by the elevators lines.
Started with the removing the elevator. This took some time as I was confused as how the bounding boxes were saved after the detection. I learned that its
 	a M-by-4 matrix where each row contains a vector in the format [x y width height], where x and y correspond to the upper left corner of the 
	bounding box, with M being how many bounding boxes there are. From this information, I would go through each bounding boxand find the center by 
	getting the half of the width and height and adding them to the x and y. From there my plan would have been to go up pixel by pixel until it 
	found a red pixel and would recursively remove the red pixels. I however ran into problems and am going to ask Michael for help next week.
I showed how the stair detection are progressing to the others, I told them that stairs would be difficult either way as most of the stairs are unique 
	and how the detector works is that it would require every type of stair possible with rotation affecting it too, and even then it might cause 
	problems as catching the net too wide would cause small things to be classified as stairs, but they were fine with how messy it could be with 
	mulitple bounding boxes, they said it would be fine, but we agreed that I should stop focusing on the stairs for now as it might need more than 
	just a few tweeks. For now, I'm sending the stair detector into the repository as its in its best shape currently.
	
For the week of 11/4
Started out the week working on the elevator detector but after me not finding a solution with how I was approaching it at the time, however, thanks to
	the help of Sarah and Michael, they seemed to have found a solution. I was planning to meet with them to discuss the problem, but the day of the meeting
	I was delayed and ended up not being able to meet with them that day, but luckily they did not seem to need my help in figuring out the solution.
For the rest of the week I was working on node removal. I was told by Sarah to use a text doc that would contain the coordinates that the users would input
	from the site so I made a doc with those parameters as well as a blank doc, so that if the user ended up not inputing any blocked passageways, I could test
	it with that. I then realized that I would have to find a way to parse through all the files in a way that would not interfere with the rest of DotGrid. I
	first tried to have the files variable equal whatever came out of RemoveNodes, but RemoveNodes makes the files into images so I had to change a few things
	around in the rest of the method. I first created a cell with equal size to the array of files, so that I could have the images go into that cell and have 
	them be processed through ConnectBuildings, getting rid of lines that would read the file's image, like in line 153. I didnt not change the SavePath
	function as it needed the adjacencymatix and not the acutal images coming out of ConnectFloors, which were the same images used in RemoveNodes. 
In the end it all worked out for RemoveNodes. I did not put it in the first DotGrid function as that is for the web version and I was not confident in
	correctly placing the function in the right place. If you want to run it yourself, comment out the first DotGrid and use DotGrid2. In my test images in the
	screenshot folder I used the parameters DotGrid({'NoDoors.png'},50,1).

For the week of 11/11
I put the RemoveNodes functionality into the web version of DotGrid as I was told by Michael adding it shouldn't effect it, I guess I was being a bit too 
	overly cautious.
I started work on making the text steps into something nicer looking. It took me some time to understand what was being said with Sarah's original version,
	as seen in the first picture for this week. It turns out, after digging through the code and testing Sarah's version, I found out the problem. So
	how she was doing coordinates was that it had a coordinate saying x,y, one saying x,1, and then 1,y, or something to that extent. I realized that
	I just needed that first coordinate so just had the method grab the three coordinates and read only the first, as while the coordinates were being
	read, they were being deleted from the list they were in so the next coordinates could be read next. that lead me to the second image for this week.
From here I did not know what to do. There is no currently objective way to say which way was north, so I currently have a line assuming that up was north
	and then saying to move through 1 unit measurements, as we don't know exactly the proportions of the images that could be put in. The third image
	shows these temporary movement descriptions. I will discuss with the rest of the group this upcomming week how to make the directions even better,
	as I feel there should be a way that I can descibe more for the directions, but I'm not sure where to go. I also plan to work next week on starting
	the training for the door detector, as I have familiarity with the cascade detector so it should not take me too long to be able to get rid of doors
	which is a thing we are hoping to be able to do for the end result, instead of using images that don't have doors already. 
 